library(SRP)
?SRP::race
?SRP::education
devtools::use_package('vtreat')
install.packages('vtreat')
devtools::use_package('vtreat')
install.packages('quadprog')
devtools::use_package('quadprog')
devtools::build_vignettes()
devtools::build()
library(SRP)
?SRP::education
devtools::build()
devtools::build_vignettes()
vignette(package=srp)
vignette(package='SRP')
vignette(package='dplyr')
vignette('dplyr', package='dplyr')
?dplyr::add_count
devtools::build()
devtools::install()
library(SRP)
?SRP::education
?poststratify
devtools::build()
library(SRP)
education
knitr::opts_chunk$set(
collapse = TRUE,
comment = "#>"
)
library(tidyverse)
library(magrittr)
#devtools::install_github('joeornstein/SRP')
library(SRP)
trainset <- SRP::vignetteData
trainset
disag_estimates <- trainset %>%
group_by(unit) %>%
summarise(disag_estimate = mean(y),
num = n())
disag_estimates
library(lme4)
model1 <- lmer(y ~ (1|x1) + (1|x2) + unit_covariate + (1|unit), data = trainset)
PSFrame <- SRP::vignettePSFrame
PSFrame
pred <- predict(model1, PSFrame, allow.new.levels = T)
mrp_estimates <- poststratify(pred, PSFrame)
mrp_estimates
pred <- predict(model1, PSFrame, allow.new.levels = T)
mrp_estimates <- poststratify(pred, PSFrame)
mrp_estimates
library(glmnet)
library(ranger)
library(kknn)
library(xgboost)
library(caret)
#Estimate HLM
hlmFormula <- y ~ (1|x1) +  (1|x2) + unit_covariate + (1|unit)
hlmModel <- lmer(hlmFormula, data = trainset)
#Tune LASSO
lasso_vars <- c("x1","x2","unit","unit_covariate")
lasso_factors <- c('x1', 'x2', 'unit') #which variables to convert to factors
trainset_lasso <- cleanDataLASSO(trainset, lasso_vars, lasso_factors)$trainset
lassoModel <- cv.glmnet(trainset_lasso, trainset$y,
type.measure = "mse")
#Tune KNN
knnFormula <- y ~ x1 + x2 + latitude + longitude + unit_covariate
knn_train <- train.kknn(knnFormula, data=trainset, kmax = 201) #Find best k (LOOCV)
k_best <- knn_train$best.parameters$k
#Tune Random Forest
forestFormula <- y ~ x1 + x2 + latitude + longitude + unit_covariate
forestModel <- ranger(forestFormula, data = trainset)
#Tune GBM
gbm_vars <- c("x1","x2","latitude","longitude","unit_covariate")
trainset_gbm <- cleanDataGBM(trainset=trainset, gbm_vars=gbm_vars)$trainset
#Create a custom 'xgb.DMatrix'. Faster computation
dtrain <- xgb.DMatrix(trainset_gbm, label = trainset$y)
#5-fold cross-validation; pick nrounds that minimizes RMSE
xgb.tune <- xgb.cv(data = dtrain,
booster = "gbtree",
objective = "reg:linear",
eval_metric = "rmse",
eta = 0.02,
nrounds = 50 / 0.02, #Lower eta -> more trees
nfold = 5,
verbose = F,
early_stopping_rounds = 20)
gbmModel <- xgboost(data = dtrain,
booster = "gbtree",
objective = "reg:linear",
eval_metric = "rmse",
eta = 0.02,
verbose = F,
nrounds = xgb.tune$best_iteration)
stackWeights <- getStackWeights(trainset = trainset,
hlmFormula = hlmFormula,
lasso_vars = lasso_vars,
lasso_factors = lasso_factors,
forestFormula = forestFormula,
knnFormula = knnFormula, k_best = k_best,
gbm_vars = gbm_vars, gbm_factors = NULL,
gbm_params = list(eta = 0.02), gbm_tune = xgb.tune,
nfolds = 5)
stackWeights %>% round(3)
PSFrame_lasso <- cleanDataLASSO(PSFrame, lasso_vars = lasso_vars, lasso_factors = lasso_factors,
new_vars_lasso = colnames(trainset_lasso))$trainset
PSFrame_gbm <- cleanDataGBM(PSFrame, gbm_vars = gbm_vars)$trainset
M1 <- predict(hlmModel, PSFrame, allow.new.levels = T)
M2 <- predict(lassoModel, newx = PSFrame_lasso, s = lassoModel$lambda.min)
M3 <- kknn(knnFormula, train = trainset, test = PSFrame, k = k_best)$fitted.values
M4 <- predict(forestModel, PSFrame)$predictions
M5 <- predict(gbmModel, PSFrame_gbm)
M <- cbind(M1,M2,M3,M4,M5) %>% as.matrix
pred <- M %*% stackWeights
#Poststratify
srp_estimates <- poststratify(pred, PSFrame)
head(srp_estimates)
true_means <- SRP::vignetteTruth
mrp_estimates %<>% set_colnames(c('unit','mrp_estimate'))
srp_estimates %<>% set_colnames(c('unit','srp_estimate'))
results <- true_means %>%
left_join(disag_estimates, by = 'unit') %>%
left_join(mrp_estimates, by = 'unit') %>%
left_join(srp_estimates, by = 'unit')
ggplot(results) + geom_point(aes(x=disag_estimate, y=true_mean)) +
xlab('Disaggregated Estimate') + ylab('True Mean') +
geom_abline(intercept = 0, slope = 1) +
theme_bw()
ggplot(results) + geom_point(aes(x=mrp_estimate, y=true_mean)) +
xlab('MRP Estimate') + ylab('True Mean') +
geom_abline(intercept = 0, slope = 1) +
theme_bw()
ggplot(results) + geom_point(aes(x=srp_estimate, y=true_mean)) +
xlab('SRP Estimate') + ylab('True Mean') +
geom_abline(intercept = 0, slope = 1) +
theme_bw()
PSFrame1 <- SRP::race
PSFrame2 <- SRP::education
PSFrame1
PSFrame2
PSFrame <- getSyntheticPSFrame(PSFrame1, PSFrame2)
PSFrame
PSFrame3 <- SRP::sex
PSFrame3
PSFrame <- getSyntheticPSFrame(PSFrame, PSFrame3)
PSFrame
devtools::build()
devtools::install()
library(SRP)
?SRP::education
devtools::install()
library(SRP)
?SRP::education
remove.packages('SRP')
devtools::install()
devtools::build()
#devtools::install_github('joeornstein/SRP')
#library(SRP)
source('../R/poststratificationFunctions.r')
source('../R/stackingFunctions.r')
#devtools::install_github('joeornstein/SRP')
#library(SRP)
source('../R/poststratificationFunctions.r')
source('../R/stackingFunctions.r')
rm(list=ls())
#trainset <- SRP::vignetteData
trainset <- load('../data/vignetteData.rda')
#trainset <- SRP::vignetteData
trainset <- load(file = '../data/vignetteData.rda')
#trainset <- SRP::vignetteData
load(file = '../data/vignetteData.rda')
trainset <- vignetteData
?load
knitr::opts_chunk$set(
collapse = TRUE,
comment = "#>"
)
library(tidyverse)
library(magrittr)
#devtools::install_github('joeornstein/SRP')
#library(SRP)
source('../R/poststratificationFunctions.r')
source('../R/stackingFunctions.r')
#trainset <- SRP::vignetteData
load(file = '../data/vignetteData.rda')
trainset <- vignetteData
trainset
disag_estimates <- trainset %>%
group_by(unit) %>%
summarise(disag_estimate = mean(y),
num = n())
disag_estimates
library(lme4)
model1 <- lmer(y ~ (1|x1) + (1|x2) + unit_covariate + (1|unit), data = trainset)
#PSFrame <- SRP::vignettePSFrame
load(file = '../data/vignettePSFrame.rda')
PSFrame <- vignetteData
PSFrame
pred <- predict(model1, PSFrame, allow.new.levels = T)
mrp_estimates <- poststratify(pred, PSFrame)
PSFrame
knitr::opts_chunk$set(
collapse = TRUE,
comment = "#>"
)
library(tidyverse)
library(magrittr)
#devtools::install_github('joeornstein/SRP')
#library(SRP)
source('../R/poststratificationFunctions.r')
source('../R/stackingFunctions.r')
#trainset <- SRP::vignetteData
load(file = '../data/vignetteData.rda')
trainset <- vignetteData
trainset
disag_estimates <- trainset %>%
group_by(unit) %>%
summarise(disag_estimate = mean(y),
num = n())
disag_estimates
library(lme4)
model1 <- lmer(y ~ (1|x1) + (1|x2) + unit_covariate + (1|unit), data = trainset)
#PSFrame <- SRP::vignettePSFrame
load(file = '../data/vignettePSFrame.rda')
PSFrame <- vignettePSFrame
PSFrame
pred <- predict(model1, PSFrame, allow.new.levels = T)
mrp_estimates <- poststratify(pred, PSFrame)
mrp_estimates
library(glmnet)
library(ranger)
library(kknn)
library(xgboost)
library(caret)
#Estimate HLM
hlmFormula <- y ~ (1|x1) +  (1|x2) + unit_covariate + (1|unit)
hlmModel <- lmer(hlmFormula, data = trainset)
#Tune LASSO
lasso_vars <- c("x1","x2","unit","unit_covariate")
lasso_factors <- c('x1', 'x2', 'unit') #which variables to convert to factors
trainset_lasso <- cleanDataLASSO(trainset, lasso_vars, lasso_factors)$trainset
lassoModel <- cv.glmnet(trainset_lasso, trainset$y,
type.measure = "mse")
#Tune KNN
knnFormula <- y ~ x1 + x2 + latitude + longitude + unit_covariate
knn_train <- train.kknn(knnFormula, data=trainset, kmax = 201) #Find best k (LOOCV)
k_best <- knn_train$best.parameters$k
#Tune Random Forest
forestFormula <- y ~ x1 + x2 + latitude + longitude + unit_covariate
forestModel <- ranger(forestFormula, data = trainset)
#Tune GBM
gbm_vars <- c("x1","x2","latitude","longitude","unit_covariate")
trainset_gbm <- cleanDataGBM(trainset=trainset, gbm_vars=gbm_vars)$trainset
#Create a custom 'xgb.DMatrix'. Faster computation
dtrain <- xgb.DMatrix(trainset_gbm, label = trainset$y)
#5-fold cross-validation; pick nrounds that minimizes RMSE
xgb.tune <- xgb.cv(data = dtrain,
booster = "gbtree",
objective = "reg:linear",
eval_metric = "rmse",
eta = 0.02,
nrounds = 50 / 0.02, #Lower eta -> more trees
nfold = 5,
verbose = F,
early_stopping_rounds = 20)
gbmModel <- xgboost(data = dtrain,
booster = "gbtree",
objective = "reg:linear",
eval_metric = "rmse",
eta = 0.02,
verbose = F,
nrounds = xgb.tune$best_iteration)
stackWeights <- getStackWeights(trainset = trainset,
hlmFormula = hlmFormula,
lasso_vars = lasso_vars,
lasso_factors = lasso_factors,
forestFormula = forestFormula,
knnFormula = knnFormula, k_best = k_best,
gbm_vars = gbm_vars, gbm_factors = NULL,
gbm_params = list(eta = 0.02), gbm_tune = xgb.tune,
nfolds = 5)
stackWeights %>% round(3)
PSFrame_lasso <- cleanDataLASSO(PSFrame, lasso_vars = lasso_vars, lasso_factors = lasso_factors,
new_vars_lasso = colnames(trainset_lasso))$trainset
PSFrame_gbm <- cleanDataGBM(PSFrame, gbm_vars = gbm_vars)$trainset
M1 <- predict(hlmModel, PSFrame, allow.new.levels = T)
M2 <- predict(lassoModel, newx = PSFrame_lasso, s = lassoModel$lambda.min)
M3 <- kknn(knnFormula, train = trainset, test = PSFrame, k = k_best)$fitted.values
M4 <- predict(forestModel, PSFrame)$predictions
M5 <- predict(gbmModel, PSFrame_gbm)
M <- cbind(M1,M2,M3,M4,M5) %>% as.matrix
pred <- M %*% stackWeights
#Poststratify
srp_estimates <- poststratify(pred, PSFrame)
head(srp_estimates)
#true_means <- SRP::vignetteTruth
load(file = '../data/vignetteTruth.rda')
true_means <- vignetteTruth
mrp_estimates %<>% set_colnames(c('unit','mrp_estimate'))
srp_estimates %<>% set_colnames(c('unit','srp_estimate'))
results <- true_means %>%
left_join(disag_estimates, by = 'unit') %>%
left_join(mrp_estimates, by = 'unit') %>%
left_join(srp_estimates, by = 'unit')
ggplot(results) + geom_point(aes(x=disag_estimate, y=true_mean)) +
xlab('Disaggregated Estimate') + ylab('True Mean') +
geom_abline(intercept = 0, slope = 1) +
theme_bw()
ggplot(results) + geom_point(aes(x=mrp_estimate, y=true_mean)) +
xlab('MRP Estimate') + ylab('True Mean') +
geom_abline(intercept = 0, slope = 1) +
theme_bw()
ggplot(results) + geom_point(aes(x=srp_estimate, y=true_mean)) +
xlab('SRP Estimate') + ylab('True Mean') +
geom_abline(intercept = 0, slope = 1) +
theme_bw()
#PSFrame1 <- SRP::race
load('..data/race.rda')
#PSFrame1 <- SRP::race
load('../data/race.rda')
PSFrame1 <- race
#PSFrame2 <- SRP::education
load('..data/education.rda')
#PSFrame1 <- SRP::race
load('../data/race.rda')
PSFrame1 <- race
#PSFrame2 <- SRP::education
load('../data/education.rda')
PSFrame2 <- education
PSFrame1
PSFrame2
PSFrame <- getSyntheticPSFrame(PSFrame1, PSFrame2)
PSFrame
#PSFrame3 <- SRP::sex
load('../data/sex.rda')
PSFrame3 <- sex
PSFrame3
PSFrame <- getSyntheticPSFrame(PSFrame, PSFrame3)
PSFrame
devtools::build_vignettes()
devtools::build()
devtools::install()
library(SRP)
library(SRP)
?SRP::education
devtools::build()
devtools::install()
library(SRP)
?SRP::education
remove.packages('SRP')
devtools::build_vignettes()
devtools::install()
?SRP::education
roxygen2::roxygenize()
library(SRP)
?SRP::education
library(SRP)
?SRP::education
knitr::opts_chunk$set(
collapse = TRUE,
comment = "#>"
)
library(tidyverse)
library(magrittr)
devtools::install_github('joeornstein/SRP')
library(SRP)
trainset <- SRP::vignetteData
trainset
disag_estimates <- trainset %>%
group_by(unit) %>%
summarise(disag_estimate = mean(y),
num = n())
disag_estimates
library(lme4)
model1 <- lmer(y ~ (1|x1) + (1|x2) + unit_covariate + (1|unit), data = trainset)
PSFrame <- SRP::vignettePSFrame
PSFrame
pred <- predict(model1, PSFrame, allow.new.levels = T)
mrp_estimates <- poststratify(pred, PSFrame)
mrp_estimates
library(glmnet)
library(ranger)
library(kknn)
library(xgboost)
library(caret)
#Estimate HLM
hlmFormula <- y ~ (1|x1) +  (1|x2) + unit_covariate + (1|unit)
hlmModel <- lmer(hlmFormula, data = trainset)
#Tune LASSO
lasso_vars <- c("x1","x2","unit","unit_covariate")
lasso_factors <- c('x1', 'x2', 'unit') #which variables to convert to factors
trainset_lasso <- cleanDataLASSO(trainset, lasso_vars, lasso_factors)$trainset
lassoModel <- cv.glmnet(trainset_lasso, trainset$y,
type.measure = "mse")
#Tune KNN
knnFormula <- y ~ x1 + x2 + latitude + longitude + unit_covariate
knn_train <- train.kknn(knnFormula, data=trainset, kmax = 201) #Find best k (LOOCV)
k_best <- knn_train$best.parameters$k
#Tune Random Forest
forestFormula <- y ~ x1 + x2 + latitude + longitude + unit_covariate
forestModel <- ranger(forestFormula, data = trainset)
#Tune GBM
gbm_vars <- c("x1","x2","latitude","longitude","unit_covariate")
trainset_gbm <- cleanDataGBM(trainset=trainset, gbm_vars=gbm_vars)$trainset
#Create a custom 'xgb.DMatrix'. Faster computation
dtrain <- xgb.DMatrix(trainset_gbm, label = trainset$y)
#5-fold cross-validation; pick nrounds that minimizes RMSE
xgb.tune <- xgb.cv(data = dtrain,
booster = "gbtree",
objective = "reg:linear",
eval_metric = "rmse",
eta = 0.02,
nrounds = 50 / 0.02, #Lower eta -> more trees
nfold = 5,
verbose = F,
early_stopping_rounds = 20)
gbmModel <- xgboost(data = dtrain,
booster = "gbtree",
objective = "reg:linear",
eval_metric = "rmse",
eta = 0.02,
verbose = F,
nrounds = xgb.tune$best_iteration)
stackWeights <- getStackWeights(trainset = trainset,
hlmFormula = hlmFormula,
lasso_vars = lasso_vars,
lasso_factors = lasso_factors,
forestFormula = forestFormula,
knnFormula = knnFormula, k_best = k_best,
gbm_vars = gbm_vars, gbm_factors = NULL,
gbm_params = list(eta = 0.02), gbm_tune = xgb.tune,
nfolds = 5)
stackWeights %>% round(3)
PSFrame_lasso <- cleanDataLASSO(PSFrame, lasso_vars = lasso_vars, lasso_factors = lasso_factors,
new_vars_lasso = colnames(trainset_lasso))$trainset
PSFrame_gbm <- cleanDataGBM(PSFrame, gbm_vars = gbm_vars)$trainset
M1 <- predict(hlmModel, PSFrame, allow.new.levels = T)
M2 <- predict(lassoModel, newx = PSFrame_lasso, s = lassoModel$lambda.min)
M3 <- kknn(knnFormula, train = trainset, test = PSFrame, k = k_best)$fitted.values
M4 <- predict(forestModel, PSFrame)$predictions
M5 <- predict(gbmModel, PSFrame_gbm)
M <- cbind(M1,M2,M3,M4,M5) %>% as.matrix
pred <- M %*% stackWeights
#Poststratify
srp_estimates <- poststratify(pred, PSFrame)
head(srp_estimates)
true_means <- SRP::vignetteTruth
mrp_estimates %<>% set_colnames(c('unit','mrp_estimate'))
srp_estimates %<>% set_colnames(c('unit','srp_estimate'))
results <- true_means %>%
left_join(disag_estimates, by = 'unit') %>%
left_join(mrp_estimates, by = 'unit') %>%
left_join(srp_estimates, by = 'unit')
ggplot(results) + geom_point(aes(x=disag_estimate, y=true_mean)) +
xlab('Disaggregated Estimate') + ylab('True Mean') +
geom_abline(intercept = 0, slope = 1) +
theme_bw()
ggplot(results) + geom_point(aes(x=mrp_estimate, y=true_mean)) +
xlab('MRP Estimate') + ylab('True Mean') +
geom_abline(intercept = 0, slope = 1) +
theme_bw()
ggplot(results) + geom_point(aes(x=srp_estimate, y=true_mean)) +
xlab('SRP Estimate') + ylab('True Mean') +
geom_abline(intercept = 0, slope = 1) +
theme_bw()
PSFrame1 <- SRP::race
PSFrame2 <- SRP::education
PSFrame1
PSFrame2
PSFrame <- getSyntheticPSFrame(PSFrame1, PSFrame2)
PSFrame
PSFrame3 <- SRP::sex
PSFrame3
PSFrame <- getSyntheticPSFrame(PSFrame, PSFrame3)
PSFrame
SRP::education
